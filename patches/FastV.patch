diff --git a/src/FastV/inference/eval/model_vqa_loader.py b/src/FastV/inference/eval/model_vqa_loader.py
index a50944d..22df24b 100644
--- a/src/FastV/inference/eval/model_vqa_loader.py
+++ b/src/FastV/inference/eval/model_vqa_loader.py
@@ -55,17 +55,24 @@ class CustomDataset(Dataset):
 
         input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')
 
-        return input_ids, image_tensor
+        return input_ids, image_tensor, image.size
 
     def __len__(self):
         return len(self.questions)
 
 
+def collate_fn(batch):
+    input_ids, image_tensors, image_sizes = zip(*batch)
+    input_ids = torch.stack(input_ids, dim=0)
+    image_tensors = torch.stack(image_tensors, dim=0)
+    return input_ids, image_tensors, image_sizes
+
+
 # DataLoader
 def create_data_loader(questions, image_folder, tokenizer, image_processor, model_config, batch_size=1, num_workers=4):
     assert batch_size == 1, "batch_size must be 1"
     dataset = CustomDataset(questions, image_folder, tokenizer, image_processor, model_config)
-    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)
+    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)
     return data_loader
 
 
@@ -74,8 +81,41 @@ def eval_model(args):
     disable_torch_init()
     model_path = os.path.expanduser(args.model_path)
     model_name = get_model_name_from_path(model_path)
-    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)
-
+    
+    tokenizer, model, image_processor, context_len = load_pretrained_model(
+        model_path, 
+        args.model_base, 
+        model_name,
+    )
+
+    # set model fastv config
+    tag = args.tag
+    weight = args.weight_file 
+
+    if args.use_fast_v == "True":
+        model.config.use_fast_v = True
+        model.config.fast_v_sys_length = args.fast_v_sys_length
+        model.config.fast_v_image_token_length = args.fast_v_image_token_length
+        model.config.fast_v_attention_rank = args.fast_v_attention_rank
+        model.config.fast_v_agg_layer = args.fast_v_agg_layer
+    else:
+        model.config.use_fast_v = False
+    model.model.reset_fastv()
+    
+    # model.model.reset_rope(use_no_rope=True)
+    
+    model.model.define_attention_map(
+        tag,
+        weight
+    )
+    
+    print("####################### related information ####################### ")
+    print("TAG:", tag)
+    print("args.fast_v_attention_rank=", args.fast_v_attention_rank)
+    print("args.fast_v_agg_layer=", args.fast_v_agg_layer)
+    print("weight_file:", weight)
+    
+    # Data
     questions = [json.loads(q) for q in open(os.path.expanduser(args.question_file), "r")]
     questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
     answers_file = os.path.expanduser(args.answers_file)
@@ -87,39 +127,40 @@ def eval_model(args):
         print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {args.conv_mode}.')
 
     data_loader = create_data_loader(questions, args.image_folder, tokenizer, image_processor, model.config)
-
-    for (input_ids, image_tensor), line in tqdm(zip(data_loader, questions), total=len(questions)):
+    data_bar = tqdm(zip(data_loader, questions), total=len(questions))
+    
+    for (input_ids, image_tensors, image_sizes), line in data_bar:
         idx = line["question_id"]
         cur_prompt = line["text"]
-
+        model.model._cached_image_size = image_sizes[0]
         input_ids = input_ids.to(device='cuda', non_blocking=True)
-
+        image_tensors = image_tensors.to(dtype=torch.float16, device='cuda', non_blocking=True)
+            
         with torch.inference_mode():
             output_ids = model.generate(
                 input_ids,
-                images=image_tensor.to(dtype=torch.float16, device='cuda', non_blocking=True),
-                do_sample=True if args.temperature > 0 else False,
+                images=image_tensors,
+                attention_mask=None,
+                # do_sample=True if args.temperature > 0 else False,
+                do_sample=False,
                 temperature=args.temperature,
-                top_p=args.top_p,
-                num_beams=args.num_beams,
                 max_new_tokens=args.max_new_tokens,
-                use_cache=True)
+                use_cache=True,
+                output_attentions=True,
+                output_scores=True,
+                return_dict_in_generate=True)
 
-        input_token_len = input_ids.shape[1]
-        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
-        if n_diff_input_output > 0:
-            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
-        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
-        outputs = outputs.strip()
+        outputs = tokenizer.decode(output_ids['sequences'][0, input_ids.shape[1]:],skip_spectial_tokens=True).strip().replace("</s>","")
 
         ans_id = shortuuid.uuid()
         ans_file.write(json.dumps({"question_id": idx,
-                                   "prompt": cur_prompt,
-                                   "text": outputs,
-                                   "answer_id": ans_id,
-                                   "model_id": model_name,
-                                   "metadata": {}}) + "\n")
-        # ans_file.flush()
+                                "prompt": cur_prompt,
+                                "text": outputs,
+                                "answer_id": ans_id,
+                                "model_id": model_name,
+                                "metadata": {}}) + "\n")
+        ans_file.flush()
+        
     ans_file.close()
 
 if __name__ == "__main__":
@@ -136,6 +177,19 @@ if __name__ == "__main__":
     parser.add_argument("--top_p", type=float, default=None)
     parser.add_argument("--num_beams", type=int, default=1)
     parser.add_argument("--max_new_tokens", type=int, default=128)
+    
+    # parser.add_argument("--visual_token_num", type=int, default=576)
+    # parser.add_argument("--important_ratio", type=float, default=0.5)
+    # fastV
+    parser.add_argument('--use-fast-v', type=str, required=True, help='whether to use fast-v')
+    parser.add_argument('--fast-v-sys-length', type=int, required=False, help='the length of system prompt')
+    parser.add_argument('--fast-v-image-token-length', type=int, required=False, help='the length of image token')
+    parser.add_argument('--fast-v-attention-rank', type=int, required=False, help='the rank of attention matrix')
+    parser.add_argument('--fast-v-agg-layer', type=int, required=False, help='the layer of attention matrix')
+
+    parser.add_argument('--tag', type=str, default="before", help='traing before or after balanced')
+    parser.add_argument('--weight-file', type=str, default=None)
+    
     args = parser.parse_args()
 
     eval_model(args)
diff --git a/src/FastV/inference/eval/model_vqa_mmbench.py b/src/FastV/inference/eval/model_vqa_mmbench.py
index 2ffec1b..c88ef0c 100644
--- a/src/FastV/inference/eval/model_vqa_mmbench.py
+++ b/src/FastV/inference/eval/model_vqa_mmbench.py
@@ -56,10 +56,40 @@ def eval_model(args):
     disable_torch_init()
     model_path = os.path.expanduser(args.model_path)
     model_name = get_model_name_from_path(model_path)
-    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)
 
+    tokenizer, model, image_processor, context_len = load_pretrained_model(
+        model_path, 
+        args.model_base, 
+        model_name
+    )
+
+    # set model fastv config
+    tag = args.tag
+    weight = args.weight_file 
+
+    if args.use_fast_v == "True":
+        model.config.use_fast_v = True
+        model.config.fast_v_sys_length = args.fast_v_sys_length
+        model.config.fast_v_image_token_length = args.fast_v_image_token_length
+        model.config.fast_v_attention_rank = args.fast_v_attention_rank
+        model.config.fast_v_agg_layer = args.fast_v_agg_layer
+    else:
+        model.config.use_fast_v = False
+    model.model.reset_fastv()
+    model.model.define_attention_map(
+        tag,
+        weight)
+    
+    print("####################### related information ####################### ")
+    print("TAG:", tag)
+    print("args.fast_v_attention_rank=", args.fast_v_attention_rank)
+    print("args.fast_v_agg_layer=", args.fast_v_agg_layer)
+    print("weight_file:", weight)
+    
+    # Data
     questions = pd.read_table(os.path.expanduser(args.question_file))
     questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
+    # questions = questions.iloc[370:]
     answers_file = os.path.expanduser(args.answers_file)
     os.makedirs(os.path.dirname(answers_file), exist_ok=True)
     ans_file = open(answers_file, "w")
@@ -68,7 +98,8 @@ def eval_model(args):
         args.conv_mode = args.conv_mode + '_mmtag'
         print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {args.conv_mode}.')
 
-    for index, row in tqdm(questions.iterrows(), total=len(questions)):
+    data_bar = tqdm(questions.iterrows(), total=len(questions))
+    for index, row in data_bar:
         options = get_options(row, all_options)
         cur_option_char = all_options[:len(options)]
 
@@ -106,32 +137,32 @@ def eval_model(args):
             input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
 
             image_tensor = process_images([image], image_processor, model.config)[0]
-            # image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
-
-            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
-
+            if tag != "paper":
+                model.model._cached_image_size = image.size
+            
             with torch.inference_mode():
                 output_ids = model.generate(
                     input_ids,
                     images=image_tensor.unsqueeze(0).half().cuda(),
-                    do_sample=True if args.temperature > 0 else False,
+                    # image_sizes=[image.size],
+                    attention_mask=None,
+                    # do_sample=True if args.temperature > 0 else False,
+                    do_sample=False,
                     temperature=args.temperature,
                     top_p=args.top_p,
                     num_beams=args.num_beams,
                     # no_repeat_ngram_size=3,
                     max_new_tokens=1024,
-                    use_cache=True)
-
-            input_token_len = input_ids.shape[1]
-            n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
-            if n_diff_input_output > 0:
-                print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
-            outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
-            outputs = outputs.strip()
-            if outputs.endswith(stop_str):
-                outputs = outputs[:-len(stop_str)]
-            outputs = outputs.strip()
-
+                    use_cache=False,
+                    output_attentions=True,
+                    output_scores=True,
+                    return_dict_in_generate=True
+                    )
+
+            # outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
+            outputs = tokenizer.decode(output_ids['sequences'][0, input_ids.shape[1]:],skip_spectial_tokens=True).strip().replace("</s>","")
+            
+                
             ans_id = shortuuid.uuid()
             ans_file.write(json.dumps({"question_id": idx,
                                     "round_id": round_idx,
@@ -165,6 +196,17 @@ if __name__ == "__main__":
     parser.add_argument("--all-rounds", action="store_true")
     parser.add_argument("--single-pred-prompt", action="store_true")
     parser.add_argument("--lang", type=str, default="en")
+    # parser.add_argument("--visual_token_num", type=int, default=576)
+    # parser.add_argument("--important_ratio", type=float, default=0.5)
+    
+    #fastV
+    parser.add_argument('--use-fast-v', type=str, required=True, help='whether to use fast-v')
+    parser.add_argument('--fast-v-sys-length', type=int, required=False, help='the length of system prompt')
+    parser.add_argument('--fast-v-image-token-length', type=int, required=False, help='the length of image token')
+    parser.add_argument('--fast-v-attention-rank', type=int, required=False, help='the rank of attention matrix')
+    parser.add_argument('--fast-v-agg-layer', type=int, required=False, help='the layer of attention matrix')
+    parser.add_argument('--tag', type=str, default="before", help='traing before or after balanced')
+    parser.add_argument('--weight-file', type=str, default=None)
     args = parser.parse_args()
 
     eval_model(args)
diff --git a/src/FastV/inference/eval/model_vqa_science.py b/src/FastV/inference/eval/model_vqa_science.py
index e99501f..202760b 100644
--- a/src/FastV/inference/eval/model_vqa_science.py
+++ b/src/FastV/inference/eval/model_vqa_science.py
@@ -9,7 +9,7 @@ from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_S
 from llava.conversation import conv_templates, SeparatorStyle
 from llava.model.builder import load_pretrained_model
 from llava.utils import disable_torch_init
-from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria
+from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path
 
 from PIL import Image
 import math
@@ -31,14 +31,46 @@ def eval_model(args):
     disable_torch_init()
     model_path = os.path.expanduser(args.model_path)
     model_name = get_model_name_from_path(model_path)
-    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)
 
+    tokenizer, model, image_processor, context_len = load_pretrained_model(
+        model_path, 
+        args.model_base, 
+        model_name
+    )
+
+    # set model fastv config
+    tag = args.tag
+    weight = args.weight_file
+
+    if args.use_fast_v == "True":
+        model.config.use_fast_v = True
+        model.config.fast_v_sys_length = args.fast_v_sys_length
+        model.config.fast_v_image_token_length = args.fast_v_image_token_length
+        model.config.fast_v_attention_rank = args.fast_v_attention_rank
+        model.config.fast_v_agg_layer = args.fast_v_agg_layer
+    else:
+        model.config.use_fast_v = False
+    model.model.reset_fastv()
+    model.model.define_attention_map(tag, weight)
+    
+    print("####################### related information ####################### ")
+    print("TAG:", tag)
+    print("args.fast_v_attention_rank=", args.fast_v_attention_rank)
+    print("args.fast_v_agg_layer=", args.fast_v_agg_layer)
+    print("weight_file:", weight)
+    
+    # Data
     questions = json.load(open(os.path.expanduser(args.question_file), "r"))
+    print(len(questions))
+    questions = [q for q in questions if "image" in q]
+    print(len(questions))
     questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
     answers_file = os.path.expanduser(args.answers_file)
     os.makedirs(os.path.dirname(answers_file), exist_ok=True)
     ans_file = open(answers_file, "w")
-    for i, line in enumerate(tqdm(questions)):
+
+    data_bar = tqdm(questions)
+    for i, line in enumerate(data_bar):
         idx = line["id"]
         question = line['conversations'][0]
         qs = question['value'].replace('<image>', '').strip()
@@ -47,15 +79,18 @@ def eval_model(args):
         if 'image' in line:
             image_file = line["image"]
             image = Image.open(os.path.join(args.image_folder, image_file))
-            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
+            image_tensor = process_images([image], image_processor, model.config)[0]
             images = image_tensor.unsqueeze(0).half().cuda()
+            image_sizes = [image.size]
             if getattr(model.config, 'mm_use_im_start_end', False):
                 qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs
             else:
                 qs = DEFAULT_IMAGE_TOKEN + '\n' + qs
-            cur_prompt = '<image>' + '\n' + cur_prompt
+            cur_prompt = '<image>' + '\n' + cur_prompt    
+            model.model._cached_image_size = image.size
         else:
             images = None
+            image_sizes = None
 
         if args.single_pred_prompt:
             qs = qs + '\n' + "Answer with the option's letter from the given choices directly."
@@ -67,58 +102,26 @@ def eval_model(args):
         prompt = conv.get_prompt()
 
         input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
-
-        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
-        keywords = [stop_str]
-        stopping_criteria = [KeywordsStoppingCriteria(keywords, tokenizer, input_ids)] if conv.version == "v0" else None
-
+        
         with torch.inference_mode():
             output_ids = model.generate(
                 input_ids,
                 images=images,
-                do_sample=True if args.temperature > 0 else False,
+                # image_sizes=image_sizes,
+                attention_mask=None,
+                # do_sample=True if args.temperature > 0 else False,
+                do_sample=False,
                 temperature=args.temperature,
                 max_new_tokens=1024,
                 use_cache=True,
-                stopping_criteria=stopping_criteria,
+                output_attentions=True,
+                output_scores=True,
+                return_dict_in_generate=True
             )
+            # data_bar.set_postfix({"visual_token_num": visual_token_num})
 
-        input_token_len = input_ids.shape[1]
-        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
-        if n_diff_input_output > 0:
-            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
-        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
-        outputs = outputs.strip()
-        if outputs.endswith(stop_str):
-            outputs = outputs[:-len(stop_str)]
-        outputs = outputs.strip()
-
-        # prompt for answer
-        if args.answer_prompter:
-            outputs_reasoning = outputs
-            input_ids = tokenizer_image_token(prompt + outputs_reasoning + ' ###\nANSWER:', tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
-
-            with torch.inference_mode():
-                output_ids = model.generate(
-                    input_ids,
-                    images=images,
-                    do_sample=True if args.temperature > 0 else False,
-                    temperature=args.temperature,
-                    max_new_tokens=64,
-                    use_cache=True,
-                    stopping_criteria=[stopping_criteria])
-
-            input_token_len = input_ids.shape[1]
-            n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
-            if n_diff_input_output > 0:
-                print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
-            outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
-            outputs = outputs.strip()
-            if outputs.endswith(stop_str):
-                outputs = outputs[:-len(stop_str)]
-            outputs = outputs.strip()
-            outputs = outputs_reasoning + '\n The answer is ' + outputs
-
+        # outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
+        outputs = tokenizer.decode(output_ids['sequences'][0, input_ids.shape[1]:],skip_spectial_tokens=True).strip().replace("</s>","")     
         ans_id = shortuuid.uuid()
         ans_file.write(json.dumps({"question_id": idx,
                                    "prompt": cur_prompt,
@@ -142,6 +145,19 @@ if __name__ == "__main__":
     parser.add_argument("--temperature", type=float, default=0.2)
     parser.add_argument("--answer-prompter", action="store_true")
     parser.add_argument("--single-pred-prompt", action="store_true")
+    # parser.add_argument("--visual_token_num", type=int, default=576)
+    # parser.add_argument("--important_ratio", type=float, default=0.5)
+    
+    #fastV
+    parser.add_argument('--use-fast-v', type=str, required=True, help='whether to use fast-v')
+    parser.add_argument('--fast-v-sys-length', type=int, required=False, help='the length of system prompt')
+    parser.add_argument('--fast-v-image-token-length', type=int, required=False, help='the length of image token')
+    parser.add_argument('--fast-v-attention-rank', type=int, required=False, help='the rank of attention matrix')
+    parser.add_argument('--fast-v-agg-layer', type=int, required=False, help='the layer of attention matrix')
+    
+    parser.add_argument('--tag', type=str, default="before", help='traing before or after balanced')
+    parser.add_argument('--weight-file', type=str, default=None)
+    
     args = parser.parse_args()
 
     eval_model(args)
diff --git a/src/transformers/src/transformers/models/llama/modeling_llama.py b/src/transformers/src/transformers/models/llama/modeling_llama.py
index 1a4b0be..cc059e1 100644
--- a/src/transformers/src/transformers/models/llama/modeling_llama.py
+++ b/src/transformers/src/transformers/models/llama/modeling_llama.py
@@ -33,12 +33,99 @@ from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast,
 from ...modeling_utils import PreTrainedModel
 from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings
 from .configuration_llama import LlamaConfig
-
+import numpy as np
 
 logger = logging.get_logger(__name__)
 
 _CONFIG_FOR_DOC = "LlamaConfig"
 
+def unpad_mask(original_size, grid_size=24):
+    """
+    Unpads a PyTorch tensor of a padded and resized image.
+
+    Args:
+    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.
+    original_size (tuple): The original size of the image (height, width).
+
+    Returns:
+    torch.Tensor: The unpadded image tensor.
+    """
+    tensor = torch.zeros(grid_size,grid_size)
+    original_width, original_height = original_size
+    current_height, current_width = tensor.shape
+
+    # Compute aspect ratios
+    original_aspect_ratio = original_width / original_height
+    current_aspect_ratio = current_width / current_height
+
+    # Determine padding size and direction
+    if original_aspect_ratio > current_aspect_ratio:
+        # Padding was added to the height
+        scale_factor = current_width / original_width
+        new_height = int(original_height * scale_factor)
+        padding = (current_height - new_height) // 2
+        tensor[padding : current_height - padding, :] = 1
+    else:
+        # Padding was added to the width
+        scale_factor = current_height / original_height
+        new_width = int(original_width * scale_factor)
+        padding = (current_width - new_width) // 2
+        tensor[:, padding : current_width - padding] = 1
+
+    return tensor
+
+def generate_pad_mask(width, height, grid_size=24, img_size=336, threshold=0.3):
+    """
+    Generates a padding mask of shape (grid_size, grid_size).
+
+    This function calculates the padding ratio for each patch in the grid.
+    If the padding ratio of a patch exceeds the threshold, the mask value is set to 0 
+    (indicating a padded/invalid region); otherwise, it is set to 1.
+
+    Args:
+        width (int): The original width of the image.
+        height (int): The original height of the image.
+        grid_size (int, optional): The number of patches per row/column. Defaults to 24.
+        img_size (int, optional): The target size of the resized image. Defaults to 336.
+        threshold (float, optional): The threshold for padding ratio. Defaults to 0.3.
+
+    Returns:
+        np.ndarray: A mask of shape (grid_size, grid_size) with integer type.
+    """
+    patch_size = img_size // grid_size
+    mask = np.zeros((grid_size, grid_size), dtype=int)
+
+    if width < height:
+        pad = (height - width) / (2 * height) * img_size
+        left, right = pad, img_size - pad
+        top, bottom = 0, img_size
+    elif height < width:
+        pad = (width - height) / (2 * width) * img_size
+        top, bottom = pad, img_size - pad
+        left, right = 0, img_size
+    else:
+        left, right, top, bottom = 0, img_size, 0, img_size
+
+    for i in range(grid_size):
+        for j in range(grid_size):
+            x0, x1 = j * patch_size, (j + 1) * patch_size
+            y0, y1 = i * patch_size, (i + 1) * patch_size
+
+            inter_left = max(x0, left)
+            inter_right = min(x1, right)
+            inter_top = max(y0, top)
+            inter_bottom = min(y1, bottom)
+
+            inter_w = max(0, inter_right - inter_left)
+            inter_h = max(0, inter_bottom - inter_top)
+            inter_area = inter_w * inter_h
+
+            patch_area = patch_size * patch_size
+            pad_ratio = 1 - inter_area / patch_area
+
+            mask[i, j] = 0 if pad_ratio > threshold else 1
+
+    return mask
 
 # Copied from transformers.models.bart.modeling_bart._make_causal_mask
 def _make_causal_mask(
@@ -316,7 +403,7 @@ class LlamaAttention(nn.Module):
             kv_seq_len += past_key_value[0].shape[-2]
         
         
-        cos, sin = self.rotary_emb(value_states, seq_len=1000)
+        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
 
         if past_key_value is not None:
@@ -587,7 +674,9 @@ class LlamaModel(LlamaPreTrainedModel):
         self.fast_v_agg_layer = self.config.fast_v_agg_layer
         self.fast_v_inplace = self.config.fast_v_inplace
 
-
+    def define_attention_map(self, tag, weight):
+        self.tag = tag
+        self.weight = weight
 
     def get_input_embeddings(self):
         return self.embed_tokens
@@ -776,11 +865,33 @@ class LlamaModel(LlamaPreTrainedModel):
                             # compute average attention over different head
                             last_layer_attention_avg = torch.mean(last_layer_attention, dim=1)[0]
                             # generate new attention mask based on the average attention, sample the top ATTENTION_RANK tokens with highest attention
-                            last_layer_attention_avg_last_tok = last_layer_attention_avg[-1]
-                            # get the attention in image token
+                            last_layer_attention_avg_last_tok = last_layer_attention_avg[-1]                            
+                            # get the attention in image token 
                             last_layer_attention_avg_last_tok_image = last_layer_attention_avg_last_tok[SYS_LENGTH:SYS_LENGTH+IMAGE_TOKEN_LENGTH]
-                            # get the indexs of the top ATTENTION_RANK tokens
-                            top_attention_rank_index = last_layer_attention_avg_last_tok_image.topk(ATTENTION_RANK).indices + SYS_LENGTH
+
+                            if self.tag == "after":
+                                if hasattr(self, "_cached_image_size"):
+                                    image_w, image_h = self._cached_image_size
+                                    attention_score_pad_mask = torch.tensor(generate_pad_mask(image_w, image_h, threshold=0.5)).to(dtype=last_layer_attention_avg_last_tok_image.dtype, device=last_layer_attention_avg_last_tok_image.device)
+                                    if (attention_score_pad_mask==1).sum() < ATTENTION_RANK:
+                                        ATTENTION_RANK = (attention_score_pad_mask==1).sum()
+                                    last_layer_attention_avg_last_tok_image = last_layer_attention_avg_last_tok_image * attention_score_pad_mask.reshape(-1)
+                                spline_weight = np.load(self.weight)
+                                last_layer_attention_avg_weight = torch.from_numpy(spline_weight).to(dtype=last_layer_attention_avg_last_tok_image.dtype, device=last_layer_attention_avg_last_tok_image.device)
+                                # last_layer_attention_avg_weight[-1] = 0
+                                last_layer_attention_avg_last_tok_image_mul_weight = last_layer_attention_avg_weight * last_layer_attention_avg_last_tok_image
+                                top_attention_rank_index = last_layer_attention_avg_last_tok_image_mul_weight.topk(ATTENTION_RANK).indices + SYS_LENGTH
+                            else:
+                                if self.tag == "before" and hasattr(self, "_cached_image_size"):
+                                    image_w, image_h = self._cached_image_size
+                                    # attention_score_pad_mask = torch.tensor(generate_pad_mask(image_w, image_h, threshold=0.2)).to(dtype=last_layer_attention_avg_last_tok_image.dtype, device=last_layer_attention_avg_last_tok_image.device)
+                                    attention_score_pad_mask = unpad_mask(self._cached_image_size).to(dtype=last_layer_attention_avg_last_tok_image.dtype, device=last_layer_attention_avg_last_tok_image.device)
+                                    if (attention_score_pad_mask==1).sum() < ATTENTION_RANK:
+                                        ATTENTION_RANK = (attention_score_pad_mask==1).sum()
+                                    last_layer_attention_avg_last_tok_image = last_layer_attention_avg_last_tok_image * attention_score_pad_mask.reshape(-1)
+                                # get the indexs of the top ATTENTION_RANK tokens
+                                top_attention_rank_index = last_layer_attention_avg_last_tok_image.topk(ATTENTION_RANK).indices + SYS_LENGTH
+                                                        
                             # generate new attention mask
                             gen_attention_mask = torch.ones((batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device)
                             gen_attention_mask[:,SYS_LENGTH:SYS_LENGTH+IMAGE_TOKEN_LENGTH] = False
